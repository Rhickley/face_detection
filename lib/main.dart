import 'dart:async';
import 'dart:isolate';
import 'dart:ui';
import 'dart:typed_data';
import 'dart:io';
import 'dart:math' as math;
import 'package:image/image.dart' as img;
import 'package:path_provider/path_provider.dart';

import 'package:tflite_flutter/tflite_flutter.dart';
import 'package:camera/camera.dart';
import 'package:flutter/foundation.dart';
import 'package:flutter/material.dart';
import 'package:flutter/services.dart';
import 'package:google_mlkit_face_detection/google_mlkit_face_detection.dart';

late List<CameraDescription> _cameras;

// Configura√ß√µes do modelo
const facialModel = 'assets/modelo_convertido.tflite';
const inputSize = 48;
const facialLabel = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise'];

const facialLabelTraduzidas = {
  'Angry': 'Raiva',
  'Disgust': 'Desgosto',
  'Fear': 'Medo',
  'Happy': 'Feliz',
  'Neutral': 'Neutro',
  'Sad': 'Triste',
  'Surprise': 'Surpresa'
};

// Classe para representar a regi√£o do rosto detectado
class FaceRegion {
  final int x;
  final int y;
  final int width;
  final int height;

  FaceRegion(this.x, this.y, this.width, this.height);
}

void main() async {
  WidgetsFlutterBinding.ensureInitialized();
  _cameras = await availableCameras();
  runApp(const MyApp());
}

class MyApp extends StatelessWidget {
  const MyApp({super.key});

  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      title: 'Detector de Emo√ß√µes',
      theme: ThemeData(
        colorScheme: ColorScheme.fromSeed(seedColor: Colors.deepPurple),
        useMaterial3: true,
      ),
      home: const MyHomePage(title: 'Detec√ß√£o de Emo√ß√µes'),
    );
  }
}

class MyHomePage extends StatefulWidget {
  const MyHomePage({super.key, required this.title});

  final String title;

  @override
  State<MyHomePage> createState() => _MyHomePageState();
}

class _MyHomePageState extends State<MyHomePage> with WidgetsBindingObserver {
  CameraController? controller;
  DateTime? lastProcessedTime;
  bool faceDetected = false;
  bool isProcessing = false;
  String detectedEmotion = "Procurando...";
  bool isFrontCamera = true;

  // Isolate para detec√ß√£o de rostos
  ReceivePort? _receivePort;
  Isolate? _isolate;
  SendPort? _sendPort;
  Orientation? _currentOrientation;
  final Completer<void> _isolateReady = Completer<void>();

  // Classificador de emo√ß√µes
  late Interpreter _interpreter;
  List<int>? _inputShape;
  List<int>? _outputShape;

  @override
  void initState() {
    super.initState();
    WidgetsBinding.instance.addObserver(this);
    _currentOrientation = _calculateCurrentOrientation();
    _initIsolate();
    _loadModel().then((_) => _initializeCamera(1));
    debugPrint("üîÑ Inicializando aplica√ß√£o...");
  }

  Future<void> _loadModel() async {
    try {
      debugPrint("‚¨áÔ∏è Iniciando carregamento do modelo...");
      final options = InterpreterOptions();

      // Habilitar XNNPACK para melhor performance
      try {
        if (Platform.isAndroid) {
          debugPrint("‚ö° Tentando habilitar XNNPACK delegate...");
          options.addDelegate(XNNPackDelegate());
          debugPrint("‚úÖ XNNPACK delegate habilitado com sucesso");
        }
      } catch (e) {
        debugPrint("‚ö†Ô∏è Falha ao habilitar XNNPACK: $e");
      }

      debugPrint("üì¶ Carregando modelo TFLite...");
      _interpreter = await Interpreter.fromAsset(facialModel, options: options);
      debugPrint("‚úÖ Modelo carregado com sucesso");

      // Obter shapes de entrada e sa√≠da
      debugPrint("üìä Obtendo tensores de entrada/sa√≠da...");
      final inputTensors = _interpreter.getInputTensors();
      final outputTensors = _interpreter.getOutputTensors();

      if (inputTensors.isNotEmpty) {
        _inputShape = inputTensors.first.shape;
        debugPrint("üì• Input shape: $_inputShape");
      } else {
        debugPrint("‚ö†Ô∏è Nenhum tensor de entrada encontrado");
      }

      if (outputTensors.isNotEmpty) {
        _outputShape = outputTensors.first.shape;
        debugPrint("üì§ Output shape: $_outputShape");
      } else {
        debugPrint("‚ö†Ô∏è Nenhum tensor de sa√≠da encontrado");
      }

      // Verificar se o shape de entrada √© compat√≠vel
      if (_inputShape == null || _inputShape!.length != 4 ||
          _inputShape![1] != inputSize ||
          _inputShape![2] != inputSize ||
          _inputShape![3] != 1) {
        debugPrint("‚ö†Ô∏è Modelo espera um input de [1, $inputSize, $inputSize, 1] mas recebeu $_inputShape");
      } else {
        debugPrint("üÜó Shape de entrada compat√≠vel");
      }
    } catch (e) {
      debugPrint("‚ùå Erro cr√≠tico ao carregar modelo: $e");
      rethrow;
    }
  }

  Future<void> _toggleCamera() async {
    debugPrint("üîÑ Alternando c√¢mera...");
    isFrontCamera = !isFrontCamera;
    await _initializeCamera(isFrontCamera ? 1 : 0);
  }

  Orientation _calculateCurrentOrientation() {
    final orientation = WidgetsBinding.instance.window.physicalSize.aspectRatio > 1
        ? Orientation.landscape
        : Orientation.portrait;
    debugPrint("üß≠ Orienta√ß√£o calculada: $orientation");
    return orientation;
  }

  @override
  void didChangeMetrics() {
    super.didChangeMetrics();
    final newOrientation = _calculateCurrentOrientation();
    debugPrint("üîÑ Mudan√ßa de orienta√ß√£o detectada: $_currentOrientation ‚Üí $newOrientation");
    setState(() {
      _currentOrientation = newOrientation;
    });
  }

  Future<void> _initializeCamera(int cameraIndex) async {
    try {
      debugPrint("üì∑ Inicializando c√¢mera $cameraIndex...");
      if (controller != null) {
        debugPrint("‚ôªÔ∏è Liberando c√¢mera anterior...");
        await controller!.dispose();
      }

      controller = CameraController(_cameras[cameraIndex], ResolutionPreset.high);
      debugPrint("‚öôÔ∏è Configurando c√¢mera...");
      await controller!.initialize();

      if (mounted) {
        debugPrint("üé• Iniciando stream de imagens...");
        controller!.startImageStream(_processCameraImage);
        setState(() {});
        debugPrint("‚úÖ C√¢mera ${isFrontCamera ? "frontal" : "traseira"} inicializada com sucesso");
      }
    } catch (e) {
      debugPrint("‚ùå Falha na inicializa√ß√£o da c√¢mera: ${e.toString()}");
      setState(() => controller = null);
    }
  }

  Future<void> _initIsolate() async {
    debugPrint("üßµ Inicializando isolate para detec√ß√£o de rostos...");
    _receivePort = ReceivePort();
    _isolate = await Isolate.spawn(
      _isolateEntry,
      _IsolateInitializationParams(
        mainSendPort: _receivePort!.sendPort,
        rootIsolateToken: RootIsolateToken.instance!,
      ),
      debugName: 'FaceDetectionIsolate',
    );

    _receivePort!.listen((message) {
      if (message is SendPort) {
        debugPrint("üì® Porta de comunica√ß√£o do isolate recebida");
        _sendPort = message;
        _isolateReady.complete();
        debugPrint("‚úÖ Isolate de detec√ß√£o de rostos inicializado com sucesso");
      } else if (message is FaceRegion) {
        debugPrint("üë§ Rosto detectado em ${message.x},${message.y} ${message.width}x${message.height}");
        setState(() => faceDetected = true);
      } else if (message == null) {
        debugPrint("‚ùå Nenhum rosto detectado");
        setState(() => faceDetected = false);
      }
    });
  }

  static void _isolateEntry(_IsolateInitializationParams params) {
    debugPrint("üßµ Isolate de detec√ß√£o iniciado");
    BackgroundIsolateBinaryMessenger.ensureInitialized(params.rootIsolateToken);
    final faceDetector = FaceDetector(
      options: FaceDetectorOptions(
        enableContours: true,
        enableLandmarks: true,
        enableClassification: false,
        performanceMode: FaceDetectorMode.accurate,
        enableTracking: true,
        minFaceSize: 0.3,
      ),
    );

    final receivePort = ReceivePort();
    params.mainSendPort.send(receivePort.sendPort);

    receivePort.listen((message) async {
      if (message is List<dynamic>) {
        debugPrint("üì∑ Processando imagem no isolate...");
        final SendPort replyPort = message[0];
        final Uint8List nv21Bytes = message[1];
        final int width = message[2];
        final int height = message[3];
        final Orientation orientation = message[4];

        try {
          final inputImage = InputImage.fromBytes(
            bytes: nv21Bytes,
            metadata: InputImageMetadata(
              size: Size(width.toDouble(), height.toDouble()),
              rotation: _getRotationFromCamera(orientation),
              format: InputImageFormat.nv21,
              bytesPerRow: width,
            ),
          );

          debugPrint("üîç Detectando rostos...");
          final faces = await faceDetector.processImage(inputImage);

          if (faces.isNotEmpty) {
            // Pegar o primeiro rosto (mais proeminente)
            final face = faces.first;
            final rect = face.boundingBox;

            // Ajustar coordenadas para garantir que est√£o dentro dos limites
            final left = rect.left.clamp(0, width.toDouble());
            final top = rect.top.clamp(0, height.toDouble());
            final right = rect.right.clamp(0, width.toDouble());
            final bottom = rect.bottom.clamp(0, height.toDouble());

            // Criar regi√£o do rosto
            final faceRegion = FaceRegion(
              left.toInt(),
              top.toInt(),
              (right - left).toInt(),
              (bottom - top).toInt(),
            );

            debugPrint("‚úÖ Rosto detectado em $left,$top ${faceRegion.width}x${faceRegion.height}");
            replyPort.send(faceRegion);
          } else {
            debugPrint("‚ùå Nenhum rosto encontrado");
            replyPort.send(null);
          }
        } catch (e) {
          debugPrint("‚ùå Erro no isolate durante detec√ß√£o: $e");
          replyPort.send(null);
        }
      }
    });
  }

  Uint8List _yuv420ToNv21(CameraImage image) {
    debugPrint("üîÑ Convertendo YUV420 para NV21...");
    final Uint8List yBuffer = image.planes[0].bytes;
    final Uint8List uBuffer = image.planes[1].bytes;
    final Uint8List vBuffer = image.planes[2].bytes;

    final int ySize = yBuffer.length;
    final int uvSize = (image.width ~/ 2) * (image.height ~/ 2);

    final Uint8List nv21 = Uint8List(ySize + uvSize * 2);
    nv21.setRange(0, ySize, yBuffer);

    int index = ySize;
    for (int i = 0; i < uvSize; i++) {
      nv21[index++] = vBuffer[i];
      nv21[index++] = uBuffer[i];
    }

    debugPrint("‚úÖ Convers√£o YUV‚ÜíNV21 conclu√≠da");
    return nv21;
  }

  Float32List _yuvToModelInput(CameraImage image, FaceRegion faceRegion) {
    final inputBuffer = Float32List(1 * inputSize * inputSize * 1);
    final yPlane = image.planes[0].bytes;
    final uPlane = image.planes[1].bytes;
    final vPlane = image.planes[2].bytes;
    final yRowStride = image.planes[0].bytesPerRow;
    final uvRowStride = image.planes[1].bytesPerRow;
    final uvPixelStride = image.planes[1].bytesPerPixel;

    // Criar uma imagem tempor√°ria para o rosto recortado
    final debugImage = img.Image(width: inputSize, height: inputSize);

    // Fatores de escala para redimensionar o rosto para inputSize
    final scaleX = faceRegion.width / inputSize;
    final scaleY = faceRegion.height / inputSize;

    // Determinar se precisa rotacionar (para portrait)
    final bool needsRotation = _currentOrientation == Orientation.portrait;
    final bool isFrontCameraRotated = isFrontCamera && needsRotation;

    int pixelIndex = 0;
    for (int y = 0; y < inputSize; y++) {
      for (int x = 0; x < inputSize; x++) {
        // Calcular coordenadas na regi√£o do rosto
        int faceX = (x * scaleX).toInt();
        int faceY = (y * scaleY).toInt();

        // Coordenadas na imagem original
        int origX = faceRegion.x + faceX;
        int origY = faceRegion.y + faceY;

        // Aplicar rota√ß√£o de 90 graus no sentido hor√°rio se necess√°rio
        if (needsRotation) {
          final temp = origX;
          origX = origY;
          origY = image.height - 1 - temp;
        }

        // Espelhar horizontalmente se for c√¢mera frontal (ap√≥s a rota√ß√£o)
        if (isFrontCameraRotated) {
          origX = image.width - 1 - origX;
        }

        // Garantir que as coordenadas est√£o dentro dos limites
        origX = origX.clamp(0, image.width - 1);
        origY = origY.clamp(0, image.height - 1);

        // Obter valor Y (lumin√¢ncia)
        final pixelValue = yPlane[origY * yRowStride + origX];

        // Normalizar para [0,1]
        inputBuffer[pixelIndex++] = pixelValue / 255.0;

        // Debug: Armazenar na imagem visualiz√°vel
        debugImage.setPixelRgb(x, y, pixelValue, pixelValue, pixelValue);
      }
    }

    _saveDebugImage(debugImage);
    return inputBuffer;
  }

  void _saveDebugImage(img.Image image) async {
    try {
      final directory = Platform.isAndroid
          ? Directory('/storage/emulated/0/Download') // Android
          : await getApplicationDocumentsDirectory(); // iOS
      final file = File('${directory.path}/debug_emotion_input.png');
      await file.writeAsBytes(img.encodePng(image));
      debugPrint('üñºÔ∏è Imagem salva em: ${file.path}');
    } catch (e) {
      debugPrint('‚ùå Erro ao salvar imagem: $e');
    }
  }

  static InputImageRotation _getRotationFromCamera(Orientation orientation) {
    final rotation = orientation == Orientation.portrait
        ? Platform.isAndroid
        ? InputImageRotation.rotation270deg
        : InputImageRotation.rotation180deg
        : InputImageRotation.rotation0deg;
    debugPrint("üîÑ Rota√ß√£o da c√¢mera: $rotation");
    return rotation;
  }

  void _processCameraImage(CameraImage image) async {
    final now = DateTime.now();
    if (lastProcessedTime != null && now.difference(lastProcessedTime!).inMilliseconds < 500) {
      debugPrint("‚è≠Ô∏è Pulando frame - processamento muito r√°pido");
      return;
    }
    if (isProcessing) {
      debugPrint("‚è≠Ô∏è Pulando frame - j√° em processamento");
      return;
    }
    if (!_isolateReady.isCompleted) {
      debugPrint("‚è≠Ô∏è Pulando frame - isolate n√£o pronto");
      return;
    }
    if (_interpreter == null) {
      debugPrint("‚è≠Ô∏è Pulando frame - modelo n√£o carregado");
      return;
    }

    isProcessing = true;
    lastProcessedTime = now;
    debugPrint("\nüîÑ Processando novo frame (${image.width}x${image.height})...");

    try {
      final replyPort = ReceivePort();
      debugPrint("üì® Enviando imagem para o isolate...");
      final nv21Bytes = _yuv420ToNv21(image);
      _sendPort!.send([
        replyPort.sendPort,
        nv21Bytes,
        image.width,
        image.height,
        _currentOrientation!,
      ]);

      replyPort.listen((message) async {
        if (message is FaceRegion) {
          debugPrint("üë§ Rosto detectado no frame em ${message.x},${message.y} ${message.width}x${message.height}");
          setState(() => faceDetected = true);

          debugPrint("üß† Preparando para an√°lise de emo√ß√µes...");
          // Convers√£o YUV para input do modelo com recorte do rosto
          final input = _yuvToModelInput(image, message);

          // Verificar se temos os shapes necess√°rios
          if (_inputShape == null || _outputShape == null) {
            debugPrint("‚ö†Ô∏è Shapes de input/output n√£o dispon√≠veis");
            setState(() => detectedEmotion = "Erro no modelo");
            return;
          }

          // Preparar buffer de sa√≠da
          final outputSize = _outputShape!.reduce((a, b) => a * b);
          final output = List.filled(outputSize, 0.0).reshape(_outputShape!);

          try {
            debugPrint("‚ö° Executando infer√™ncia do modelo...");
            _interpreter.run(input.reshape(_inputShape!), output);

            debugPrint("üìä Sa√≠das do modelo: ${output[0]}");

            // Encontrar emo√ß√£o com maior confian√ßa
            int maxIndex = 0;
            double maxConfidence = output[0][0];
            for (int i = 1; i < output[0].length; i++) {
              if (output[0][i] > maxConfidence) {
                maxConfidence = output[0][i];
                maxIndex = i;
              }
            }

            String emotion = facialLabel[maxIndex];
            String emotionTranslated = facialLabelTraduzidas[emotion] ?? 'Desconhecido';
            String confidence = (maxConfidence * 100).toStringAsFixed(1);

            debugPrint("üé≠ Emo√ß√£o detectada: $emotionTranslated ($confidence%)");
            setState(() => detectedEmotion = "$emotionTranslated ($confidence%)");
          } catch (e) {
            debugPrint("‚ùå Erro durante infer√™ncia: $e");
            setState(() => detectedEmotion = "Erro na detec√ß√£o");
          }
        } else {
          debugPrint("üëÄ Nenhum rosto - resetando emo√ß√£o");
          setState(() {
            faceDetected = false;
            detectedEmotion = "Procurando...";
          });
        }
        replyPort.close();
        isProcessing = false;
        debugPrint("‚úÖ Processamento do frame conclu√≠do");
      });
    } catch (e) {
      debugPrint("‚ùå Erro cr√≠tico no processamento: $e");
      isProcessing = false;
    }
  }

  @override
  void dispose() {
    debugPrint("‚ôªÔ∏è Liberando recursos...");
    WidgetsBinding.instance.removeObserver(this);
    _interpreter.close();
    _isolate?.kill(priority: Isolate.immediate);
    _receivePort?.close();
    controller?.dispose();
    super.dispose();
    debugPrint("‚úÖ Recursos liberados");
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      appBar: AppBar(
        title: Text(widget.title),
      ),
      body: Stack(
        children: [
          if (controller != null && controller!.value.isInitialized)
            CameraPreview(controller!),
          _buildDetectionIndicator(),
        ],
      ),
      floatingActionButton: FloatingActionButton(
        onPressed: _toggleCamera,
        tooltip: 'Alternar c√¢mera',
        child: const Icon(Icons.switch_camera),
      ),
    );
  }

  Widget _buildDetectionIndicator() {
    return Positioned(
      bottom: 50,
      left: 0,
      right: 0,
      child: Center(
        child: AnimatedContainer(
          duration: const Duration(milliseconds: 200),
          padding: const EdgeInsets.all(12),
          decoration: BoxDecoration(
            color: faceDetected ? Colors.green : Colors.red,
            borderRadius: BorderRadius.circular(8),
          ),
          child: Text(
            faceDetected ? "Emo√ß√£o: $detectedEmotion" : "Procurando rosto...",
            style: const TextStyle(color: Colors.white, fontSize: 18),
          ),
        ),
      ),
    );
  }
}

class _IsolateInitializationParams {
  final SendPort mainSendPort;
  final RootIsolateToken rootIsolateToken;
  const _IsolateInitializationParams({
    required this.mainSendPort,
    required this.rootIsolateToken,
  });
}